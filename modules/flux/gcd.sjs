/* (c) 2013-2019 Oni Labs, http://onilabs.com
 *
 * This file is part of Conductance.
 *
 * It is subject to the license terms in the LICENSE file
 * found in the top-level directory of this distribution.
 * No part of Conductance, including this file, may be
 * copied, modified, propagated, or distributed except
 * according to the terms contained in the LICENSE file.
 */

@ = require('mho:std');
var { instantiate, isSimpleType, IdToKey, KeyToId, KeyToParentKey, KeyToKind, cotraverse } = require('./schema');
var { Context } = require('./gcd/backend');
var { ChangeBuffer, structuralClone } = require('./helpers');

/**
  @nodoc
  @hostenv nodejs
*/

/*
BATCH PERIODS set the time (in ms) over which to batch calls to the GCD backend

Why it makes sense to set a non-zero call batch period:

  With a batch period set to 0 (the default for @fn.unbatched), 
  only 'temporally adjacent' calls will batched into a single request, i.e. calls that don't have a
  hold(0) (or longer) in between them. 

  Several library functions, such as each.par/transform.par, etc, have
  hold(0)'s built-in to limit recursion depth. 
  In a call such as

    data .. @transform.par(50, datum -> server.foo(datum)) .. ...

  there will be a built-in hold(0) for every 10's concurrent invocation of 
  server.foo.
  Thus a zero batch period would cause only 10 server.foo calls to be batched up 
  into the same request, and not 50 as the code might suggest. 
*/
var READ_BATCH_PERIOD    = 100;
var WRITE_BATCH_PERIOD   = 20;
var READ_T_BATCH_PERIOD  =  0; // read batch period inside transaction
var WRITE_T_BATCH_PERIOD =  0; // write batch period inside transaction
var MAX_CONCURRENT_LOOKUPS = 300; // if this is too high, we might get deferred results
                                  // on lookup, which we can handle just fine, but it is
                                  // more performant to prevent them in the first place

/**
   @class Entity
   @summary Document me!
*/

//----------------------------------------------------------------------
// Google Cloud Datastore 

// Serialized key: kind.name/kind.name/...
// kind and name must not contain slashes or dots.
// kinds and names matching the regexp __.*__ are reserved by GCD
// name must not start with '#' (unless generated by the db - see GCD docs).

// Example:

// key:      'Company.Oni Labs/Person.Alexander Fritze'
// parent:   'Company.Oni Labs'
// id:       'Alexander Fritze'
// kind:     'Person'

__js function keyToGCDKey(path) {
  try {
    return path.split('/') ..
      @map(function(pelem) {
        var split = pelem.split(':');
        var kind = split[0];
        var id_or_name = split[1];
        if (!id_or_name)
          return { kind: kind };
        else if (id_or_name.charAt(0) == '#')
          return { kind: kind, id: id_or_name.substr(1) }
        else
          return { kind: kind, name: id_or_name }
      });
  }
  catch (e) {
    throw new Error("Invalid Google Cloud Datastore key '#{path}'");
  }
}

__js function GCDKeyToKey(path) {
  return path .. 
    @transform(p -> p.name ? "#{p.kind}:#{p.name}" : "#{p.kind}:##{p.id}") ..
    @join('/');
}

__js function GCDKeyToId(path) {
  var last = path[path.length-1];
  return last.id ? "##{last.id}" : last.name;
}

__js function GCDKeyToKind(path) {
  var last = path[path.length-1];
  return last.kind;
}

__js function GCDKeyToParent(path) {
  return GCDKeyToKey(path .. @slice(0,-1));
}


__js function GCDValueToJSValue(gcd_value, descriptor) {
  var base_val;
  switch (descriptor.__type) {
  case 'date':
    if (gcd_value.timestampValue === undefined)
      base_val = null;
    else
      base_val = new Date(gcd_value.timestampValue);
    break;
  case 'bool':
    // for backwards-compatibility we use integerValue instead of booleanValue; older versions of GCD didn't have a separate boolean type
    if (gcd_value.integerValue === undefined)
      base_val = null;
    else
      base_val = Boolean(Number(gcd_value.integerValue));
    break;
  case 'integer':
    if (gcd_value.integerValue === undefined)
      base_val = null;
    else
      base_val = Number(gcd_value.integerValue);
    break;
  case 'float':
    if (gcd_value.doubleValue === undefined)
      base_val = null;
    else
      base_val = Number(gcd_value.doubleValue);
    break;
  case 'string':
  case 'text':
    if (gcd_value.stringValue === undefined)
      base_val = null;
    else
      base_val = gcd_value.stringValue;
    break;
  case 'ref':
    if (gcd_value.keyValue === undefined)
      base_val = null;
    else
      base_val = GCDKeyToKey(gcd_value.keyValue.path);
    break;
  default:
    throw new Error("Schema error: Unknown type #{descriptor.__type}");
  }
/*
  if (descriptor.fromRepository)
    base_val = descriptor.fromRepository(base_val);
*/
  return base_val;
}

function JSValueToGCDValue(js_val, descriptor, path) {
  var value = {};

/*  if (descriptor.toRepository)
    js_val = descriptor.toRepository(js_val);
*/
  if (js_val === null) {
    if (descriptor.__allowNull) {
      value.nullValue = null;
      return value;
    }
    else
      throw new Error("Invalid 'null' value on a #{descriptor.__type} property without '__allowNull'#{path ? ": "+path : ""}");
  }
  
  switch (descriptor.__type) {
  case 'date':
    value.timestampValue = js_val.toISOString();
    break;
  case 'bool':
    // for backwards-compatibility we use integerValue instead of booleanValue; older versions of GCD didn't have a separate boolean type
    value.integerValue = Boolean(js_val) ? 1 : 0;
    break;
  case 'integer':
    value.integerValue = Number(js_val);
    if (isNaN(value.integerValue)) throw new Error("Invalid value for numeric field");
    break;
  case 'float':
    value.doubleValue = Number(js_val);
    if (isNaN(value.doubleValue)) throw new Error("Invalid value for numeric field");
    break;
  case 'text':
    value.excludeFromIndexes = true;
    // fall through to string
  case 'string':
    value.stringValue = String(js_val);
    break;
  case 'ref':
    value.keyValue = {path:keyToGCDKey(js_val)};
    break;
  default:
    throw new Error("Unknown type #{descriptor.__type}");
  }
  return value;
}

// utility for JSEntityToGCDEntity
__js function checkUndefinedArrayMembers(dest, node, gcd_entity) {
  if (node.value === undefined) {
    // either ALL values must be undefined:
    if(dest.property.arrayValue.values.length > 0) {
      throw new Error("Google Cloud Datastore: Undefined values in arrays not supported (#{node.path})");
    }
    dest.has_undefined_members = true;
    // if an array has undefined members, we don't want to store the property
    // at all (storing it as an empty list will cause all values to come back
    // as `null`, rather than `undefined`).
    delete gcd_entity.properties[dest.name];
  } else {
    // .. or else ALL values must be defined:
    if(dest.has_undefined_members) {
      throw new Error("Google Cloud Datastore: Undefined values in arrays not supported (#{node.path})");
    }
  }
};


function JSEntityToGCDEntity(js_entity, schemas) {
  var schema = schemas[js_entity.schema];
  if (!schema) throw new Error("Unknown schema #{js_entity.schema}");
  var key = js_entity.id;
  var primary, parent = schema.__parent;
  var gcd_entity = { properties:{} };

  js_entity.data .. cotraverse(schema) {
    |node, descriptor|
    if (descriptor.type === 'object') {
      // feed through parent context (or create new one at the root)
      node.state = node.parent_state || {};
      // Since array sub-properties are stored independently,
      // we use an empty object here, and let the property-level
      // checks enforce non-sparseness (see comment below).
      if (node.state.arr_ctx && node.value === undefined) {
        node.value = {};
      }
    }
    else if (descriptor.type === 'key' && node.value !== undefined) {
      if (key && node.value !== key) 
        throw new Error("Google Cloud Datastore: Inconsistent key values in data ('#{key}' != '#{node.value}')");
      key = node.value;
    }
    else if (descriptor.type === 'primary') {
      if (node.value === undefined) throw new Error("Google Cloud Datastore: 'primary' property without value");
      if (primary !== undefined && node.value !== primary) throw new Error("Google Cloud Datastore: Inconsistent values for 'primary' property ('#{primary}' != '#{node.value}')");
      primary = node.value;
    }
    else if (descriptor.type === 'parent') {
      if (node.value === undefined) throw new Error("Google Cloud Datastore: 'parent' property without value");
      if (parent !== undefined && node.value !== parent) throw new Error("Google Cloud Datastore: Inconsistent values for 'parent' property ('#{parent}' != '#{node.value}')");
      parent = node.value;
    }
    else if (descriptor.type === 'array') {
      if (node.parent_state.arr_ctx)
        throw new Error("Google Cloud Datastore: Nested arrays not supported (#{node.path})");
      node.state = Object.create(node.parent_state);
      node.state.arr_ctx = {};
    }
    else if (node.parent_state.arr_ctx) {
      // we're in an array

      // We store structed array data in separate, parallel
      // multi-valued properties.  For this scheme to work, every
      // field of a structured array value must have a value (possibly
      // 'Null'), otherwise the correspondance between array fields
      // will get out of sync. 
      //
      // The one exception is when _all_ values are `undefined`, we can simply
      // save an empty array. (TODO: can we skip saving an array entirely?)
      //
      // (actually, we _could_ allow arrays where there is some data followed by
      // one or more `undefined` entries at the end. But the existing behaviour is
      // bad enough for data-dependent errors, so we won't implement that until
      // it's needed)

      var dest;
      if (node.parent_state.arr_ctx[descriptor.path]) {
        dest = node.parent_state.arr_ctx[descriptor.path];
      } else {
        dest = {};
        dest.has_undefined_members = false;
        dest.name = descriptor.path.replace(".[]", "");
        dest.property = {
          arrayValue : {
            values: []
          }
        };
        node.parent_state.arr_ctx[descriptor.path] = dest;
        gcd_entity.properties[dest.name] = dest.property;
      }

      dest .. checkUndefinedArrayMembers(node, gcd_entity);

      if(node.value !== undefined) {
        dest.property.arrayValue.values.push(JSValueToGCDValue(node.value, descriptor.value, node.path));
      }
    }
    else if (node.value !== undefined) {
      // a single-valued property:
      gcd_entity.properties[node.path] = 
        JSValueToGCDValue(node.value, descriptor.value, node.path);
    }
  }

  if (!key) {
    key = parent ? parent + '/' : '';
    key += js_entity.schema;
  }
  else if (parent !== undefined) {
    if (KeyToParentKey(key) !== parent)
      throw new Error("Google Cloud Datastore: 'parent' property ('#{parent}') inconsistent with key ('#{key}')");
  }

  if (primary !== undefined) {
    var id = KeyToId(key);
    if (!id) 
      key += ':'+primary;
    else if (id != primary)
      throw new Error("Google Cloud Datastore: 'primary' property inconsistent with key ('#{primary}' != '#{id}')");
  }

  gcd_entity.key = { path: keyToGCDKey(key) };

  return gcd_entity;
}
exports._JSEntityToGCDEntity = JSEntityToGCDEntity;


__js function GCDEntityToJSEntity(gcd_entity, js_entity, schemas) {
  // clone relevant parts of js_entity:
  js_entity = { id: js_entity.id, schema: js_entity.schema };

  if (!gcd_entity) {
    js_entity.data = null;
    return js_entity;
  }
  var kind = GCDKeyToKind(gcd_entity.key.path);
  var schema = schemas[kind];
  if (!schema) throw new Error("Google Cloud Datastore: Unknown schema #{kind}");

  var js_data = instantiate(schema);

  // go through properties of GCD entity:
  @ownPropertyPairs(gcd_entity.properties || {}) .. @each {
    |[name, value]|
    // find schema descriptor:
    var descriptor=schema, path=[], array_path;

    var split = name.split('.');

    for (var i=0; i<split.length; ++i) {
      var n = split[i];
      descriptor = descriptor[n];
      if (!descriptor) break; 
      if (@isArrayLike(descriptor) || descriptor.__type === 'array') {
        if (array_path) throw new Error("Invalid schema: nested arrays");
        path = path.concat(n);
        array_path = [];
        descriptor = @isArrayLike(descriptor) ? descriptor[0] : descriptor.__elems;
      }
      else {
        if (array_path) {
          array_path = array_path.concat(n);
        }
        else {
          path = path.concat(n);
        }
      }
    }

    if (!descriptor) continue;

    // make sure path exists in js_data:
    var target = path .. @slice(0,-1) .. 
      @reduce(js_data, (target, p) -> target[p] ? target[p] : (target[p] = {}));

    if (!array_path) {
      // a single value
      target[path .. @at(-1)] = GCDValueToJSValue(value, descriptor);
    }
    else {
      if (!target[path .. @at(-1)])
        target = target[path .. @at(-1)] = [];
      else
        target = target[path .. @at(-1)];
      ((value.arrayValue||{}).values||[]) .. @indexed .. @each { 
        |[i,v]|
        if (!array_path.length)
          target[i] = GCDValueToJSValue(v, descriptor);
        else {
          var base = target[i];
          if (!base) base = target[i] = {};
          base = array_path .. @slice(0, -1) .. 
            @reduce(base, (base, p) -> base[p] ? base[p] : (base[p] = {}));

          base[array_path .. @at(-1)] = GCDValueToJSValue(v, descriptor);
        }
      }
    }
  }

  // now go through schema and pick up special properties (such as
  // __key), check if required properties are there, (and fill in default properties?)
  @ownPropertyPairs(schema) .. @each {
    |[name, descriptor]|
    if (js_data[name]) continue; // already there
    if (!descriptor) continue; // a property that's explicitly set to 'undefined' or 'null'
    var base_val;
    switch (descriptor.__type) {
    case 'primary':
      base_val = GCDKeyToId(gcd_entity.key.path);
      break;
    case 'key':
      base_val = GCDKeyToKey(gcd_entity.key.path);
      break;
    case 'parent':
      base_val = GCDKeyToParent(gcd_entity.key.path);
      break;
    default:
//XXX      if (descriptor.__required)
//        throw new Error("Schema validation error: Missing value for property #{name}");
    }
    if (base_val !== undefined) {
      js_data[name] = base_val;
    }
  }

  var id = GCDKeyToKey(gcd_entity.key.path);
  if (js_entity.id) {
    if (js_entity.id !== id) 
      throw new Error("Id mismatch: #{id} != #{js_entity.id}");
  }
  else
    js_entity.id = id;

  if (js_entity.schema) {
    if (js_entity.schema !== kind)
      throw new Error("Schema mismatch: #{kind} != #{js_entity.schema}");
  }
  else 
    js_entity.schema = kind;

  js_entity.data = js_data;

  return js_entity;
}
exports._GCDEntityToJSEntity = GCDEntityToJSEntity;

/**
   @class GoogleCloudDatastore
   @summary Google Cloud Datastore object
   @function GoogleCloudDatastore
   @param {Object} [attribs]
   @attrib {Object} [schemas]
   @attrib {Object} [context] Object with settings as described at [./gcd/backend::Context] (Note: this should not be a `Context` object itself, but a hash of settings that will be passed to to [./gcd/backend::Context]!)
 */

function GoogleCloudDatastore(attribs) {
  var CHANGE_BUFFER_SIZE = 100; // XXX should this be configurable?
  var change_buffer = ChangeBuffer(CHANGE_BUFFER_SIZE);
  var context = Context(attribs.context);
  var schemas = attribs.schemas;

  function writeInner(entities, transaction) {
    // XXX we need to catch errors for individual entities here and
    // feed them through to the corresponding `write` call. ATM we
    // just fail the whole batch if one entity errors.

    var written = [];
    var deletes = [], autoid_inserts = [], updates = [], inserts = [];
    
    entities .. @each {
      |entity|
      if (entity.data === null && entity.id) {
        // DELETE
        deletes.push({path: keyToGCDKey(entity.id)});
        written.push({id: entity.id, schema: KeyToKind(entity.id)});
      }
      else if (entity.data) {
        // UPDATE/AUTOID_INSERT/CREATE
        var gcd_entity = JSEntityToGCDEntity(entity, schemas);
        
        // check if the entity has a full key path, or if we need to
        // create an autoid:
        var last = gcd_entity.key.path[gcd_entity.key.path.length-1];
        var has_path = (last.id || last.name);
        if (has_path) {
          if (entity.id) {
            if (entity.forceInsert)
              inserts.push(gcd_entity);
            else
              updates.push(gcd_entity);
            written.push({id:gcd_entity.key.path .. GCDKeyToKey, 
                          schema: GCDKeyToKind(gcd_entity.key.path)});
          }
          else {
            // the js entity didn't specify an id; one was generated
            // from a 'primary' attribute. we interpret this as an insert,
            // rather than an update/upsert, so we don't overwrite anything
            // unintentionally. To update data with a primary field, we require
            // the caller to provide an id (and the primary field).
            inserts.push(gcd_entity);
            written.push({id:gcd_entity.key.path .. GCDKeyToKey,
                          schema: GCDKeyToKind(gcd_entity.key.path)});
          }
        }
        else {
          // need to generate autoids
          autoid_inserts.push(gcd_entity);
          // id needs to be resolved later:
          written.push(null);
        }
      }
      else if (entity.id)
        throw new Error("Cannot write entity #{entity.id}: Missing 'data' field");
      else
        throw new Error("Malformed entity '#{require('sjs:debug').inspect(entity)}'");
    }
    
    // we've now got our entities sorted into `deletes`,
    // `autoid_inserts`, `updates` and `inserts`.
    // further processing depends on whether we're in a transaction or
    // in a blind write
    
    if (transaction) {
      // we're in a transaction. don't actually perform the write. just
      // return data for later comittal.
      if (autoid_inserts.length) {
        var req = { keys: autoid_inserts .. @map({key} -> key) };
        var results = context.allocateIds(req);
        // splice results into 'written'
        results.keys .. @consume {
          |next_id|
          for (var i=0;i<written.length; ++i) {
            if (written[i] === null) {
              var id = next_id().path;
              written[i] = { id: id .. GCDKeyToKey,
                             schema: id .. GCDKeyToKind }; 
            }
          }
        }
        
        // splice results into autoid_inserts
        @zip(results.keys, autoid_inserts) .. @each { 
          |[key, entity]|
          entity.key = key;
        }
        
        inserts = inserts.concat(autoid_inserts);
      }
      return { written: written, deletes: deletes, updates: updates, inserts: inserts };
    }
    else {
      // blind write
      var mutations = [];
      inserts .. @each { |insert| mutations.push({insert:insert}); }
      autoid_inserts .. @each { |insert| mutations.push({insert:insert}); }
      updates .. @each { |update| mutations.push({update:update}); }
      deletes .. @each { |del| mutations.push({'delete':del}); }
      
      var result = context.blindWrite({mutations: mutations});

      if (autoid_inserts.length) {
        // splice auto ids into return array
        var auto_id_index = 0;
        for (var i=0; i<written.length; ++i) {
          if (written[i] !== null) continue;
          var key = result.mutationResult.insertAutoIdKey[auto_id_index++].path;
          written[i] = {
            id:  key .. GCDKeyToKey,
            schema: key .. GCDKeyToKind
          };
        }
      }
      change_buffer.addChanges(written);
      return written;
    }
  }

  function readInner(entities, transaction) {
//    console.log("GCD: read #{entities.length} entities");

    //var SW = @Stopwatch('gcd_read');

    var unresolved = {};
    var duplicates = []; // array of duplicate queries [ [ primary_index, duplicate_index ], ... ]
    entities .. @indexed .. @each {
      |[i,entity]|
      if (unresolved[entity.id] !== undefined) {
        duplicates.push([unresolved[entity.id][0], i]);
      }
      else {
        try {
          unresolved[entity.id] = [i, keyToGCDKey(entity.id)];
        }
        catch(e) {
          entities[i] = new Error("Invalid key '#{entity.id}'");
        }
      }
    }

    //SW.snapshot('prep') .. console.log;

    var outstanding = unresolved .. @ownKeys .. @count;
    while (1) {
      var query = unresolved .. @ownValues .. 
        @take(MAX_CONCURRENT_LOOKUPS) .. @map([i,gcd_key] -> {path: gcd_key});

      //SW.snapshot('prep2') .. console.log;
      var results = (transaction || context).lookup({keys: query});

      //SW.snapshot('data') .. console.log;
      (results.found || []) .. @each {
        |result|
        var key = GCDKeyToKey(result.entity.key.path);
        var index = unresolved[key][0];
        if (index === undefined) 
          throw new Error("Unexpected result in google cloud datastore lookup");
        delete unresolved[key];
        try {
          entities[index] = GCDEntityToJSEntity(result.entity, entities[index], schemas);
        }
        catch (e) {
          entities[index] = e;
        }
      };
      (results.missing || []) .. @each {
        |result|
        var key = GCDKeyToKey(result.entity.key.path);
        // mark entity as missing (set data = null) and clear out of unresolved
        var index = unresolved[key][0];
        entities[index] = GCDEntityToJSEntity(null, entities[index], schemas);
        delete unresolved[key];
      }
      var new_outstanding = unresolved .. @ownKeys .. @count;
      if (new_outstanding === 0) 
        break;
      if (new_outstanding >= outstanding)
        throw new Error("GCD lookup not making progress (#{new_outstanding} outstanding from #{entities.length})");
      outstanding = new_outstanding;
//      if (results.deferred)
//        console.log(" #{results.deferred.length} deferred results");
      //SW.snapshot('resolve')  .. console.log;
    }

    duplicates .. @each {
      |[from, to]|
//      console.log("Multiple concurrent reads for same id #{entities[from].id} (#{from} -> #{to})");
      entities[to] = entities[from] .. structuralClone;
    }

    //SW.snapshot('end') .. console.log;
    return entities;
  }

  var QUERY_OPERATORS = {
    eq:  'EQUAL',
    lt:  'LESS_THAN',
    lte: 'LESS_THAN_OR_EQUAL',
    gt:  'GREATER_THAN',
    gte: 'GREATER_THAN_OR_EQUAL'
  };

  function queryInner(query_descriptor, transaction) {
    //console.log("GCD:query(#{query_descriptor .. require('sjs:debug').inspect})");
    var idsOnly = true; // XXX should we have a mode for returning
                        // full results too? It would complicate
                        // implementation of filters, etc
    var schema = schemas[query_descriptor.schema];
    if (!schema) throw new Error("Unknown schema #{query_descriptor.schema}");
    
    var kind = query_descriptor.kindless ? null : query_descriptor.schema;
    
    var filters = [];
    var sorts = {};
    var key = query_descriptor.id;
    
    if (query_descriptor.data) {
      cotraverse(query_descriptor.data, schema) {
        |node, descriptor|
        if (node.value === undefined) continue;
        if (descriptor.type === 'key') {
          if (key && node.value !== key) 
            throw new Error("Google Cloud Datastore: Inconsistent key values in data ('#{key}' != '#{node.value}')");
          key = node.value;
        }
        else if (isSimpleType(descriptor.type) || descriptor.type === 'ref') {
          if (node.value !== null && typeof(node.value) === 'object') {
            @ownKeys(QUERY_OPERATORS) .. @filter(op -> node.value[op] !== undefined) .. @each {
              |op|
              filters.push({
                propertyFilter: {
                  property: {name: descriptor.path.replace('.[]','')},
                  op: QUERY_OPERATORS[op],
                  value: JSValueToGCDValue(node.value[op], descriptor.value)
                }
              });
            }
            if (node.value.sort) {
              var direction;
              if (node.value.direction !== undefined) {
                direction = (node.value.direction < 0) ? 'DESCENDING' : 'ASCENDING';
              }
              else
                direction = 'ASCENDING';
              sorts[node.value.sort] = { property: {name: descriptor.path.replace('.[]','')},
                                         direction: direction };
            }
          }
          else 
          filters.push({
            propertyFilter: {
              property: {name: descriptor.path.replace('.[]','')},
              op: 'EQUAL',
              value: JSValueToGCDValue(node.value, descriptor.value)
            }
          });
        }
        else if (descriptor.type !== 'object' && descriptor.type !== 'array') 
          throw new Error("Unsupported query (#{node.path} = #{node.value}))");
      }
    }
    
    if (key !== undefined) {
      filters.push({
        propertyFilter: {
          property: {name: '__key__'},
          op: 'EQUAL',
          value: { keyValue: { path: keyToGCDKey(key) } }
        }
      });
    }
    else if (schema.__parent) {
      filters.push({
        propertyFilter: {
          property: {name: '__key__'},
          op: 'HAS_ANCESTOR',
          value: { keyValue: { path:keyToGCDKey(schema.__parent) } }
        }
      });
    }
    //console.log(filters .. require('sjs:debug').inspect(false, 10));
    var batchStream = @Stream(function(r) {
      // construct query request:
      var request = {query: {}};
      
      if (filters.length == 1) {
        request.query.filter = filters[0];
      }
      else if (filters.length > 1) {
        request.query.filter = {
          compositeFilter: {
            op: 'AND',
            filters: filters
          }
        };
      }
      
      if (kind) {
        request.query.kind = [{name:kind}];
      }
      
      // XXX does it make sense to hardcode this default limit (e.g. 500) here?
      request.query.limit = query_descriptor.limit || 500;

      sorts = sorts .. @ownPropertyPairs .. @sortBy('0') .. @map([,prop] -> prop);
      if (sorts.length) {
        request.query.order = sorts;
      }
      
      if (idsOnly) {
        request.query.projection = [ { property:{name:'__key__'}}];
      }
      
      //        console.log(require('sjs:debug').inspect(request, false, 20));
      
      while (1) {
        var results = (transaction || context).runQuery(request);
        if (!results.batch || !results.batch.entityResults) break;
        //console.log("GCD:query: #{results.batch.entityResults.length} results");
        r(results.batch.entityResults .. 
          @map({entity} -> idsOnly ?
              { id:GCDKeyToKey(entity.key.path), 
                schema: GCDKeyToKind(entity.key.path) } :
              GCDEntityToJSEntity(entity, {}, schemas)                
             ));
        
        // unfortunately GCD currently doesn't return
        // 'NO_MORE_RESULTS', but conservatively returns
        // MORE_RESULTS_AFTER_LIMIT, even if there aren't any further
        // results. This means we always do a redundant last query
        // see https://groups.google.com/d/msg/gcd-discuss/iNs6M1jA2Vw/kn7VVgxQeHkJ
        //console.log(results.batch.moreResults);
        if (results.batch.moreResults === 'NO_MORE_RESULTS' || !results.batch.endCursor || !results.batch.entityResults.length) break;
        request.query.startCursor = results.batch.endCursor; 
      }
    });
    
    // buffer(1) ensures that we already perform the next GCD
    // request while processing results; this is to compensate for
    // GCD's high latency
    return batchStream .. @buffer(1); 
  }

  // returns a 'Datastore' object
  var rv = {
    /**
       @function GoogleCloudDatastore.write
       @param {Object} [entity] [::Entity] object to create/update/delete
       // XXX ?
       @return {Array} Array of `{id, schema}` descriptors of changed entities
       @summary Create/update/delete entities in the datastore
       @desc
          If `entity` is of the form
          
              {  id:     IDENTIFIER_STRING,
                 schema: SCHEMA_NAME,
                 data:   SIMPLE_JS_OBJ
              }

          then the item identified by `id` will be updated with `data` as described by the
          given `schema`. If the item does not exist yet, a new item will be created.

          If `entity` is of the form
 
              {  schema: SCHEMA_NAME,
                 data:   SIMPLE_JS_OBJ
              }

          then a new item with the given `data` as described by `schema` will be created.
          
          If `entity` is of the form

              {  id: IDENTIFIER_STRING,
                 data: null
              }
               
          then the given item will be deleted from the datastore.
     */
    write: @fn.unbatched(writeInner, {batch_period: WRITE_BATCH_PERIOD}),

    /**
       @function GoogleCloudDatastore.read
       @param {::Entity} [entity] Entity to retrieve
       @return {::Entity} *entity* with `data` attribute set to retrieved data 
               or `null` if the entity could not be found in the datastore
       @summary Read an entity from the datastore
       @desc
          `entity` needs to have an `id` attribute. If `entity` contains a `schema` 
          attribute it will be compared against the stored schema of the retrieved data. 
          If the two don't match an error will be thrown.

          Other attributes on *entity* (e.g. `data`) will be ignored.
          
     */
    read: @fn.unbatched(readInner, {batch_period: READ_BATCH_PERIOD}),

    /**
       @function GoogleCloudDatastore.query
       @param {Object} [query_descriptor] Query Descriptor
       @return {sjs:sequence::Stream} Stream of result *batches* (use [sjs:sequence::unpack] to get a stream of results)
       @summary Query for entities
     */
    query: function(query_descriptor) {
      //console.log("GCD QUERY");
      return queryInner(query_descriptor);
    },

    /**
       @function GoogleCloudDatastore.withTransaction
       @altsyntax googleclouddatastore.withTransaction([options]) { |transaction| ... }
       @param {optional Object} [options] Transaction options
       @param {Function} [block]
       @summary Execute *block* with a new [::Transaction] object. The transaction will automatically be rolled back if there is an error during committing.


       @class Transaction
       @summary Object provided by [::GoogleCloudDatastore::withTransaction]
       @function Transaction.read
       @summary see [::GoogleCloudDatastore::read]
       @function Transaction.write
       @summary see [::GoogleCloudDatastore::write]
       @function Transaction.query
       @summary see [::GoogleCloudDatastore::query]

     */
    withTransaction: function(options, block) {
      if (!block) {
        block = options;
        options = {};
      }

      // we retry transactions until they succeed. block needs to be idempotent for this!!
      while (1) {
        context.withTransaction(options.isolationLevel) {
          |transaction_context|

          var mutated_ids = {}, deletes = [], updates = [], inserts = [];
          
          var read_from_db = @fn.unbatched(function(entities) {
            return readInner(entities, transaction_context);
          }, {batch_period: READ_T_BATCH_PERIOD});
          
          var rv;
          var doCommit = true;
          var api = {
            read:  function(entity) {
              // check if entity is among those mutated:
            if (mutated_ids[entity.id]) {
              // XXX the deletes/updates/inserts could be better
              // structured to facilitate this search
              if (deletes .. @find({path} -> GCDKeyToKey(path) === entity.id, undefined)) {
                entity = {id:entity.id, schema:entity.schema, data: null};
              }
              else {
                var gcd_entity = @combine(updates, inserts) .. @find({key:{path}} -> GCDKeyToKey(path) === entity.id);
                entity = gcd_entity .. GCDEntityToJSEntity(entity, schemas);
              }
              return entity;
            }
              
              // ... else not found in mutated set; we need to query the db:
              
              // XXX maybe cache items read from db
              return read_from_db(entity);
            },
            
            write: @fn.unbatched(function(entities) { 
              var result = writeInner(entities, transaction_context);
              
              // handle multiple mutations of the same record in the same transaction:
              result.written .. @each { 
                |{id}|
                if (mutated_ids[id]) {
                  // xxx the deletes/updates/inserts could be structured
                  // in a different way to make this code easier
                  var idx;
                  if (([idx] = @indexed(updates) ..  
                       @find([,{key:{path}}] -> GCDKeyToKey(path) === id, [-1])) != -1) {
                    console.log("Warning: repeated mutation of entity #{id} during transaction.");
                    // remove existing entry in 'updates' array; it will
                    // be replaced by the value from 'result.updates'
                    // later below:
                    updates.splice(idx,1);
                    
                  }
                  else 
                    throw new Error("Unsupported type of repeated mutation of  #{id} during transaction");
                }
                else
                  mutated_ids[id] = true;
              }
              
              deletes = deletes.concat(result.deletes);
              updates = updates.concat(result.updates);
              inserts = inserts.concat(result.inserts);
              
              return result.written;
            }, {batch_period: WRITE_T_BATCH_PERIOD}),
            
            query: function(entities) {
              // XXX this query should be amended to take into account the 
              // updates, inserts and deletes that were performed in the transaction 
              return queryInner(entities, transaction_context);
            },
            
            withTransaction: function() { throw new Error("GCD doesn't support nested transactions"); }
            
          };
          
          try {
            rv = block(api);
          } catch(e) {
            doCommit = false;
            throw e;
          } finally {
            if (doCommit) {
              // commit deletes, updates and inserts
              var mutations = [];
              inserts .. @each { |insert| mutations.push({insert:insert}); }
              updates .. @each { |update| mutations.push({update:update}); }
              deletes .. @each { |del| mutations.push({'delete':del}); }
              
              try {
                transaction_context.commit({mutations: mutations});
              }
              catch (e) {
                // retry the whole transaction for contention or server issues:
                if (e.statusCode >= 500 ||
                    e.statusCode == 409 ||
                    e.statusCode == 0) {
                  //console.log('transaction failed; retrying');
                  continue;
                }
                // else pass exception to caller
                throw e;
              }
              change_buffer.addChanges(@ownKeys(mutated_ids) .. @map(id -> { id: id, schema: id .. KeyToKind }));
              return rv;
            }
          }
        }
      }
    },

    /**
       @function GoogleCloudDatastore.watch
       @altsyntax gcd.watch { |changed_items| ... }
       @param {Function} [watcher] Function which will be called with arrays of `{id, kind}` descriptors of records that have changed.
       @summary Watch for changes to records.
    */
    watch: function(f) {
      var start_revision = change_buffer.revision, current_revision;
      while (true) {
        current_revision = change_buffer.dispatcher.receive();
        while (current_revision !== start_revision) {
          f(change_buffer.getChanges(start_revision));
          start_revision = current_revision;
          current_revision = change_buffer.revision;
        }
      }
    }


  };

  return rv;
}
exports.GoogleCloudDatastore = GoogleCloudDatastore;
